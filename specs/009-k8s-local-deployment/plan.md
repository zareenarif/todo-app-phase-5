# Implementation Plan: Local Kubernetes Deployment

**Branch**: `009-k8s-local-deployment` | **Date**: 2026-02-09 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/009-k8s-local-deployment/spec.md`

## Summary

Deploy the Phase 3 Todo AI Chatbot (Next.js frontend + FastAPI backend)
to a local Minikube Kubernetes cluster using production-grade multi-stage
Docker images and Helm charts. All infrastructure artifacts are
AI-generated. The existing application logic is preserved unchanged;
this plan covers only containerization, orchestration, and deployment.

## Technical Context

**Language/Version**: Dockerfile (multi-stage), Helm templates (Go templating), YAML manifests
**Primary Dependencies**: Docker Desktop, Minikube, Helm 3, kubectl, kubectl-ai, kagent
**Storage**: Neon PostgreSQL (external to cluster, connection via K8s Secret)
**Testing**: `helm lint`, `kubectl apply --dry-run`, manual E2E verification
**Target Platform**: Local Minikube Kubernetes cluster (Linux containers)
**Project Type**: Web application (frontend + backend, separately containerized)
**Performance Goals**: Pods reach Running state within 120s of `helm install`
**Constraints**: Local-only, no cloud, no CI/CD, no service mesh, no ingress controller
**Scale/Scope**: Single Minikube cluster, 1 replica each (configurable via values.yaml)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| # | Gate | Status | Evidence |
|---|------|--------|----------|
| I | Specification before implementation | PASS | spec.md approved with 18 FRs, 10 SCs |
| II | Planning before coding | PASS | This plan document |
| III | Tasks before execution | PENDING | tasks.md to be generated via /sp.tasks |
| IV | Simplicity over complexity | PASS | Using standard K8s resources (Deployment, Service, ConfigMap, Secret), Helm defaults, no custom operators |
| V | Scope discipline | PASS | Local Minikube only, no cloud/CI/CD/mesh/ingress |
| VI | Security by design | PASS | Secrets in K8s Secrets, non-root containers, pinned base images |
| VII | Stateless server | PASS | Deployments (not StatefulSets), external DB, no pod-local state |
| VIII | MCP tool discipline | PASS | Application logic unchanged, containerization only |
| IX | Conversation persistence | PASS | External Neon PostgreSQL, survives pod restarts |
| X | AI agent boundaries | PASS | Application logic unchanged |
| XI | AI-generated infrastructure only | PASS | All Dockerfiles, Helm charts, manifests generated by Claude Code |
| XII | Helm as single source of truth | PASS | All K8s resources defined in Helm templates, values.yaml config surface |
| XIII | Observable and resilient | PASS | Liveness/readiness probes, resource limits, stdout logging |

**Gate Result: ALL PASS — proceed to Phase 0.**

## Project Structure

### Documentation (this feature)

```text
specs/009-k8s-local-deployment/
├── plan.md              # This file
├── research.md          # Phase 0: tooling research
├── data-model.md        # Phase 1: infrastructure entity model
├── quickstart.md        # Phase 1: deployment quickstart
├── contracts/           # Phase 1: Helm values contract
└── tasks.md             # Phase 2: /sp.tasks output
```

### Source Code (repository root)

```text
docker/
├── frontend/
│   └── Dockerfile       # Multi-stage Next.js build
└── backend/
    └── Dockerfile       # Multi-stage FastAPI build

helm/
└── todo-chatbot/
    ├── Chart.yaml
    ├── values.yaml
    ├── .helmignore
    └── templates/
        ├── _helpers.tpl
        ├── NOTES.txt
        ├── namespace.yaml
        ├── frontend-deployment.yaml
        ├── frontend-service.yaml
        ├── backend-deployment.yaml
        ├── backend-service.yaml
        ├── configmap.yaml
        └── secret.yaml

docs/
└── deployment.md        # Step-by-step deployment guide
```

**Structure Decision**: Infrastructure-only addition at repository root.
`docker/` holds Dockerfiles (separate from existing `backend/Dockerfile`
and `frontend/Dockerfile` which are dev-oriented). `helm/` holds the
Helm chart. Existing `backend/` and `frontend/` source directories are
unchanged.

## Phase 0: Research

### R1: Docker Multi-Stage Build Strategy

**Decision**: Use multi-stage builds for both frontend and backend.

**Frontend (Next.js):**
- Stage 1 (`deps`): `node:18-alpine` — install npm dependencies
- Stage 2 (`builder`): Copy deps, run `next build` with `output: 'standalone'`
- Stage 3 (`runner`): `node:18-alpine` — copy standalone output, run as non-root
- Rationale: Next.js standalone output produces a minimal server bundle (~50MB vs ~500MB full node_modules)
- Requirement: `next.config.js` MUST set `output: 'standalone'`

**Backend (FastAPI):**
- Stage 1 (`builder`): `python:3.12-slim` — install system deps + pip packages
- Stage 2 (`runner`): `python:3.12-slim` — copy installed packages, run as non-root
- Rationale: Single slim base avoids Alpine musl issues with psycopg2-binary

**Alternatives Considered:**
- Single-stage builds: Rejected (larger images, includes build tools in runtime)
- Alpine for Python: Rejected (psycopg2-binary requires glibc, would need source compile)
- Distroless: Rejected (debugging difficulty in local dev, overkill for Minikube)

### R2: Minikube Image Loading Strategy

**Decision**: Use `minikube image load` to load locally-built images.

**Rationale**: Simplest approach — build images with Docker Desktop,
then load directly into Minikube's container runtime. No registry
needed.

**Steps:**
1. `docker build -t todo-frontend:1.0.0 -f docker/frontend/Dockerfile ./frontend`
2. `docker build -t todo-backend:1.0.0 -f docker/backend/Dockerfile ./backend`
3. `minikube image load todo-frontend:1.0.0`
4. `minikube image load todo-backend:1.0.0`

**Alternatives Considered:**
- `eval $(minikube docker-env)`: Rejected (requires building inside Minikube's Docker daemon, confusing for beginners)
- Local registry: Rejected (unnecessary complexity for local-only deployment)

### R3: Helm Chart Architecture

**Decision**: Single umbrella chart `todo-chatbot` with templates for
frontend and backend (no subcharts).

**Rationale**: Application is simple (2 deployments, 2 services,
1 configmap, 1 secret). Subcharts add complexity without benefit.
All resources share the same namespace and configuration context.

**Alternatives Considered:**
- Separate charts per service: Rejected (increases deploy commands, complicates shared config)
- Subcharts: Rejected (unnecessary for 2-service application)

### R4: Service Exposure Strategy

**Decision**: Frontend exposed via NodePort, Backend via ClusterIP.

- Frontend NodePort: Accessible from host via `minikube service`
- Backend ClusterIP: Only accessible within cluster (frontend→backend)
- Frontend references backend via K8s internal DNS:
  `http://<backend-service>.<namespace>.svc.cluster.local:8000`

**Rationale**: Only the frontend needs external access. Backend is an
internal service. This follows least-privilege networking.

**Alternatives Considered:**
- Both NodePort: Rejected (backend doesn't need external access)
- Ingress: Rejected (constitution explicitly excludes ingress controllers)

### R5: Health Check Strategy

**Decision**: Use existing endpoints for probes.

**Backend:**
- Liveness: `GET /health` (returns `{"status": "healthy"}`)
- Readiness: `GET /health` (same endpoint; confirms app is ready)
- Initial delay: 15s, period: 10s, failure threshold: 3

**Frontend:**
- Liveness: TCP socket check on port 3000
- Readiness: HTTP GET on `/` (returns 200 when Next.js is ready)
- Initial delay: 20s, period: 10s, failure threshold: 3

**Rationale**: Backend already has `/health` endpoint. Frontend
health is confirmed by TCP connectivity and page serve readiness.

### R6: Namespace Strategy

**Decision**: Deploy to namespace `todo-chatbot`.

**Rationale**: Isolates application resources from default namespace.
Helm chart creates the namespace if it doesn't exist.

### R7: Resource Limits

**Decision**: Conservative defaults suitable for Minikube.

| Container | CPU Request | CPU Limit | Memory Request | Memory Limit |
|-----------|-------------|-----------|----------------|--------------|
| Frontend  | 100m        | 500m      | 128Mi          | 512Mi        |
| Backend   | 200m        | 1000m     | 256Mi          | 1Gi          |

**Rationale**: Minikube typically has 2-4 CPUs and 4-8GB RAM. These
limits leave room for Kubernetes system components while ensuring
application stability.

### R8: Next.js Standalone Configuration

**Decision**: Modify `next.config.js` to add `output: 'standalone'`.

**Rationale**: Required for multi-stage Docker build. The standalone
output bundles only the necessary server files, drastically reducing
the production image size. This is the only application code change
required for Phase 4.

**Impact**: Non-breaking change. Standalone mode produces a
self-contained server that behaves identically to the default
Next.js server.

## Phase 1: Design

### Infrastructure Entity Model

See [data-model.md](./data-model.md) for full infrastructure entity
definitions.

### Helm Values Contract

See [contracts/helm-values.md](./contracts/helm-values.md) for the
complete values.yaml schema and configuration surface.

### Deployment Quickstart

See [quickstart.md](./quickstart.md) for step-by-step deployment
instructions.

## Execution Flow

### Step 1: Prerequisites Verification

1. Verify Docker Desktop is installed and running
2. Verify Minikube is installed (`minikube version`)
3. Verify Helm is installed (`helm version`)
4. Verify kubectl is installed (`kubectl version`)
5. Start Minikube cluster: `minikube start --cpus=2 --memory=4096`
6. Verify cluster: `kubectl cluster-info`

### Step 2: Application Configuration Change

1. Update `frontend/next.config.js` to add `output: 'standalone'`
   (only application code change in Phase 4)

### Step 3: Docker Image Build

1. Build frontend image:
   `docker build -t todo-frontend:1.0.0 -f docker/frontend/Dockerfile ./frontend`
2. Build backend image:
   `docker build -t todo-backend:1.0.0 -f docker/backend/Dockerfile ./backend`
3. Verify images: `docker images | grep todo-`
4. Load into Minikube:
   `minikube image load todo-frontend:1.0.0`
   `minikube image load todo-backend:1.0.0`

### Step 4: Kubernetes Secrets Setup

1. Create namespace: `kubectl create namespace todo-chatbot`
2. Create secrets (operator provides actual values):
   ```
   kubectl create secret generic todo-secrets \
     --namespace todo-chatbot \
     --from-literal=DATABASE_URL='<neon-postgres-url>' \
     --from-literal=BETTER_AUTH_SECRET='<auth-secret>' \
     --from-literal=OPENAI_API_KEY='<openai-key>' \
     --from-literal=GROQ_API_KEY='<groq-key>'
   ```

### Step 5: Helm Deployment

1. Lint chart: `helm lint helm/todo-chatbot/`
2. Dry-run: `helm install todo-chatbot helm/todo-chatbot/ --namespace todo-chatbot --dry-run`
3. Install: `helm install todo-chatbot helm/todo-chatbot/ --namespace todo-chatbot`
4. Verify rollout:
   `kubectl rollout status deployment/todo-chatbot-frontend -n todo-chatbot`
   `kubectl rollout status deployment/todo-chatbot-backend -n todo-chatbot`

### Step 6: Verification

1. Check pods: `kubectl get pods -n todo-chatbot`
2. Check services: `kubectl get svc -n todo-chatbot`
3. Access frontend: `minikube service todo-chatbot-frontend -n todo-chatbot`
4. Check backend health: `kubectl exec -n todo-chatbot <pod> -- curl -s localhost:8000/health`
5. Check logs: `kubectl logs -n todo-chatbot -l app=todo-chatbot`

### Step 7: Resilience Verification

1. Delete backend pod: `kubectl delete pod -n todo-chatbot -l component=backend`
2. Watch recreation: `kubectl get pods -n todo-chatbot -w`
3. Verify functionality after recreation

### Step 8: AI DevOps Operations

1. kubectl-ai: Query pod status with natural language
2. kubectl-ai: Scale deployment via natural language
3. kagent: Run cluster health analysis

### Step 9: Upgrade and Rollback

1. Modify values (e.g., replica count) in values.yaml
2. Upgrade: `helm upgrade todo-chatbot helm/todo-chatbot/ -n todo-chatbot`
3. Verify new state
4. Rollback: `helm rollback todo-chatbot 1 -n todo-chatbot`
5. Verify rollback

## Complexity Tracking

No constitution violations. All infrastructure uses standard
Kubernetes resources and Helm patterns.
